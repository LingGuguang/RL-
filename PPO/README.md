论文开篇劈了TRPO很复杂，计算KL散度时要算hassian矩阵，很费时间，且无法融合一些好技术，像添加噪声以增加探索度，在策略网络和价值网络上参数共享等。
PPO给了一个新目标，叫clipped probability rations，说它形成了一个悲观估计，也就是找了个估计的下界。
为了优化策略，会在两种行动上来回横跳：1.在策略上采样.2.在已经采过的数据上执行几个epoch。

在7个连续控制任务上，除了有一个swimmer-v1没有TRPO好，其他的都略优于所有模型或者大幅领先。
在atari游戏上，只拿了A2C和ACER两个模型比较，是因为这两个是当时效果最好的吗？我还没看到这两个论文。

公式(7)给出了clipped probability rations的形式。
原来clipped是截断的意思。在采样的时候，算出来的A可正可负，所以有两种截断方法。
A为正时，说明动作选的好，更新方向正确，但为了限制步长，我们在r > 1+epsilon时不让它提高。
A为负时，说明动作选的不好，更新方向错误，模型应加强学习，所以我们在比例过小(r < 1-epsilon)时要提高比例。

论文还给出了另一种方法，把TRPO里目标和约束用对偶近似点梯度下降法写到一起，省去了每次单独计算KL散度的麻烦，还给出了超参数的变化方法。

大家把clip法叫做PPO2，另一种叫做PPO1.
