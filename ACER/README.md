ACER的背景是A3C,actor critic和Retrace(λ)

actor critic是老朋友了
A3C也讲过了
Retrace(λ)缓解了return-base off-policy中importance sampling时系数方差太大的问题, 先讲这个:
Retrace论文：Safe and efficient off-policy reinforcement learnin
1. return-base: 以Qlearning为例，早期我们的目标函数中只有当前and往后一步的Q，而实际上后一步Q还能展开成再后后后很多步，对于完整的一条路径，这样才比较合理。return-base就是基于这种展开。
2. importance sampling的系数：是 π分布/µ分布 。π分布指下一时刻分布，µ分布是当前时刻的。如果差距小还好说，如果差距大，那不同action造成的π分布和µ分布的比值可是天差地别，这种差距直接造成了不同训练数据间学习率的巨大差距，直接表现就是"方差大"。
因此，论文给了几个方案间的对比(第4页表格)：
    1. 固定二者的比值为λ。只适合π和µ分布差距不大时。这方法纯比较用，因为差距不大我还换你干嘛。
    2. 用λ*π分布代替原先的比值。这是2000年的方法，论文说"The corresponding operator defines a contraction mapping for any policies π and µ"，相当于给原式做了个收缩映射，因为把µ给去了，整体值下降了，λ作为均匀的放缩可以先不考虑影响，总之整体的大小趋势没有改变。
    但是缺少了对μ采样的利用很可惜，而且还会在分布变化不大时cut the trace。cut是什么意思暂时不知道[TODO]。 
    3. Retrace(λ)=公式。公式去论文里看，这里打不出来。
    它综合考虑了方差大and方差小两个方面。当方差小时，保持公式不变，毕竟原装的是最好的。当方差大时，就给它拉成统一的λ。


<!-- 首先搞懂ACER的目标：
它想将梯度合理化。由于off-policy需要使用importance sampling让采样无偏，就继承了该方法天然存在的弊病：当前后策略差距大时，会破坏off-policy成立的假设：策略变化不大。
所以全文从对策略的梯度g入手，一步步添加佐料，试图通过控制学习率等方法，自适应地从多个角度减轻弊病。
ACER做了以下几个事：
1. importance weight truncation with bias corrcetion
    做法简单的硬性截断方法。不是单纯的设上限，而是给超出上限的部分一个特别设置的学习率，这样两部分都有了学习率的上界，还保证了无偏。
    该方法保证了重要性采样时合理的方差。如果方差大，则在学习率的角度来说，大的那些更能影响模型的参数变化，就像是他们比正常情况多采了好几倍的样本一样，那这样肯定造成了分布的失真。
2. 1中截断后的两部分Q，用两种估计方法。前一个用Retrace的方法，后一个用普通的值方法。
    1. Retrace方法：单指用该方法估计Qret。Q的迭代公式很简单，里面有最新的策略函数参与。现在我们知道，陈旧的数据(s,a)与最新的策略之间有代沟，将其用*截断重采样*修改后，因为截断的操作，导致了Q与真实Q产生差距。为了弥补这一差距，我们需要计算V与PQ的差。这里的P指截断重采样。
    2. 值方法：就是直接拿Q网络算出来的Q值往里面带。
3. DQN发展出的dueling network，也加入了该模型。该方法是简单的修改，而且被证实是有效果的。
    但是对于连续action，还没有相匹配的变种。所以论文给出了stochastic dueling network。这部分需要dueling network前置知识，我还没看[Todo]。
4. average policy network.
    TRPO利用KL为限制，但KL计算麻烦，所以做他的泰勒展开，可获得二阶展开，被称为Fisher Information Matrix，FIM中间有一个hessian矩阵，也很难算，但可以用共轭梯度法在有限次数内拟合到最优点。

    无论怎么优化，TRPO算hessian矩阵还是很恶心人的。所以该论文用average policy network更新参数θa。该方式利用之前的θa和最新更新的θ，所以唯一的更新的变量是θ。KL依靠θa和θ得出，既然θa固定，那KL的变化就是个一阶项，加上约束，很容易求最优解。

    TRPO利用KL散度+最优化L获得更新梯度。
    该论文想最优化的目标是g和z。g是经过一系列针对off-policy的改造得到的关于θ的梯度公式。由于Φ(θ)是构成g中策略的参数，所以g可以先对Φ(θ)求梯度，g再与dΦ(θ)/dθ相乘，结果就是g关于θ的梯度。
    但是g不管怎么优化都是有偏的，真正的关于Φ(θ)的导数z，应该满足:1.z与g的二范数最小。2.KL散度对Φ(θ)求导构成线性约束，这里没看懂z是怎么来的[TODO]。
    总之利用KKT条件，可以获得最优的z*。[TODO，这里怎么来的也没懂]
    将其用于更新θ，θ再更新到θa，一轮更新就完成了。 -->


2021.4.3 对ACER重新梳理
一开始，g作为纯粹的参数θ的梯度，设计为式(1)。该方法是重采样后的梯度，而且用A=Q-V而不是Q，因为减去V这个均值是我们发现的可以减小方差的好手段，该手段在ACER后面也用上了。
有了A，自然有了Q。通常Q都是r+φV_(t+1)，相当于往后估了一步。但是还有一种叫做n-steps的方法，就是多估几步，这样对现实的建模更合理，也符合一次游戏是从头到尾的。r就用衰减γ来降低后续r对当前的影响，以保证收敛。于是有了式(2)。
当走的n-steps多了，会发现V(t+k)前面的系数由于k次方的存在，导致值很小，小到不值得浪费时间计算。于是有了式(3)。还没完事。
式(3)前面加上了ρ，是重采样的权重，注意是累乘形式。为什么这样搞呢？如果前后两个策略分布差距很大，ρ的累乘会很小。累乘最大的情况，就是两个分布一样。而分布差距大时，是我们担心模型不稳定的时候。所以ρ的累乘就是为了减小此时的学习率。虽然该公式是无偏的，但ρ的引进只是为了减小数字上的学习率，横向对比中，学习率之间的方差巨大。因此论文后面对ρ做了处理，后面再讲。
式(4)是之前有人尝试解决方差问题给出的marginal value functions，可以看到没有n-steps，ρ也不搞累乘了，Q用最新的π生成，因为作者不想给你确切的估计Q的方法，该式子主要是告诉你怎么边缘化。然后用积分计算当前时刻所有x和a的期望。其中，x是服从当前策略μ下终点时刻x的分布。所以marginal就是t的边缘化[TODO，这里真的不知道为什么这样做，应该有理论依据，但我不知道]。
边缘化减小了ρ对方差的负面影响，接下来就是对Qπ的估计了。
2012年，有人用lambda returns来估计Qπ。该公式很有意思，R不直接从V来，而是包含了一部分下一步的R，这种"记忆"的做法还挺常见，如果你把R_(t+1)也按式子展开，能发现该式子有点像n-steps的感觉，但是无论是r的衰减加权，还是各个时刻散落的V，都有种"给定一个生成的路径，按照一定衰减将t时刻之后的R叠加为t时刻的R"的感觉。这种形式因为包含了既定路径下每个action后的state，所以特别像tree backup，而tree backup又是一个用了就可以避免importance sampling的好方法，是不是就侧面说明了该估计方法的好处呢(暂且存疑)？
论文对lambda returns方法的λ取值做了探讨，决定让λ=1，R就是Qret，形成了式(5)。ret就是return，Qret代表当前时刻之后的返回给当前时刻的value，主要成分是r，其次是各个时刻的TD-error(就是V-Q的那一堆像是误差的东西)。
为了得到Q和V，论文还设计了dueling network的连续版： Stochastic Dueling Networks (SDNs)，它可以在action连续时通过采样让计算进行下去。
p.s.[TODO]如果有人知道Qret的确切原因，请告诉我。
现在，Qret是Qπ的合理估计。
然后就是Retrace的重要性截断方法代替ρ，不是很难，因为没有理论需要学，只是个硬性的一刀切。(在第4页最上面)
直接跳到式(7)。可以看到两个除了带ρ的部分外一模一样的期望，因为他们改良了Retrace。Retrace一刀切后，被切掉的部分该论文没有扔掉，补上了第二个期望，不扔东西，g就无偏，而且两部分都可控，第一个权重最大是c，第二个最大是1.
将Qret带入(7)中前一部分的Qπ，后一部分的Qπ用网络算出的Q值代替，就有了式(8)，它为了严谨用了期望形式。
给一条根据策略μ采样出的序列，我们就能计算g了。哦，别忘了V，之前说过，它是降低方差的好朋友，相当于减去了Q的均值。我们在前后两部分Q上都-V，这就来到了式(9).
现在，我们有了一个均值方差都处理得不错的g，叫做g_acer.它是关于θ的梯度。
我们做一个小小的改变，让g先作为φ_θ的导数。我们知道，θ是生成策略，即分布的参数，而分布是靠均值方差形容的。我们先对φ求梯度，然后再求φ对θ的梯度，那么两次梯度的乘法就是g对θ的梯度。这样做是为了[TODO]。前一次就像是后一次梯度的学习率一样的存在，而且数据是μ来的，Q拟合的是π，自然就想到了用TRPO限制两者不会差太远。
我们设真实的梯度是z，所以在估计的g和z之间做均方差，让其最小，约束就用KL。[这段没看懂，z怎么被约束的？约束是怎么来的？]
根据KKT，能得到最优的z*,它作为对θ的梯度的步长，参与进θ的更新。
但是但是，我们用的θ不是这个θ，而是只把最新的θ按比例与θa相加，称为average policy network.
一次更新结束。