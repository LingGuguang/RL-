DQN对action的选择是离散的，它可数。
而很多场景是需要一个连续值。例如力度。
你可以把力度条分割的足够细，把连续值当成离散值做，但终究只是权宜之计。

DDPG就是为了解决这个问题。
它对连续策略的发展模型是从PG到DPG再到DDPG。

1. PG:policy gradient
它不是基于value，不是基于Q值，而是基于策略。
这么做的原因是，你无论基于什么，目的都是为了找个最优策略。
如果你看完了DQN里对Noisy network的描述，应该知道，你可以根据输入state，经过网络训练出均值U和方差θ，以获得一个对某个值的正态分布。多维正态分布就是高斯分布。
如果我把这个分布看作是对action值的分布，那我们就可以在连续域上对action采样了。这就完成了理论上从离散到连续的转变。
实操上是先初始化网络，然后根据state经过网络获得均值U和方差θ，然后采样action，再把action应用到环境里获得新state，直到游戏结束。就这样多采几条样，采够了就拿这些数据去训练网络，获得新网络。然后重新再采样。
如果你看完了强化学习基础（北大张志华主讲），就会在第32节课左右学到。

2. DPG: deterministic policy gradient
DPG是对PG思路的延伸。
如果你看完了强化学习基础（北大张志华主讲），你应该知道，历史随机的最优决策与马尔可夫确定的最优决策是一样的。
所以完全可以在一个更小的解域上找解。
因此产生了引入deterministic的想法。
对于分布的确定策略，就是选均值作为策略。那么计算均值的网络就变成了计算action的网络。
另外还有个根据action和state计算value(对未来的期望)的网络。
而且，这个架构与actor-critic是一样的。actor负责选择action，critic负责评判action，评判分就是value。
论文给出了on-policy和off-policy的两套参数更新公式。

on和off是指两个更新策略。
在目标函数的更新公式上，我们总能看到Q_t和Q_t+1两个时刻的Q值。
如果这两个Q是从同一个网络，或叫同一个策略里生成的，那就是on-policy。
而往往，Q_t的数据是事先生成好的，只有Q_t+1的数据是根据最新更新的网络寻找的。此时两个时刻的Q并不从同一个网络里生成。这就是off-policy。
on-policy有个毛病，当初始网络对Q估计有偏差时，由于自己与自己比较，会让偏差的现象一直存在。这种现象在DQN中存在过，后用target network缓解。
DPG同样可以用这个方法，用了就是off-policy。

3. DDPG：CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING
它借鉴了DQN的改造，例如target network, double Q-learning，且用更复杂的深度神经网络代替DPG中简单的网络。
DDPG自己设计了很多复杂的小游戏，让DDPG玩。你可以给网络喂state的参数，也可以直接喂游戏画面，所以选择什么网络就看你自己了。

除此外，DDGP加入了一个增强网络探索性的东西：Ornstein-Uhlenbeck process
DQN中，用epsilon-greedy来加强前期的探索能力，后又有其他方法给网络参数加扰动，以试图加强探索能力。
但它的方法粗暴，很乱。我们希望模型既有探索能力，又保持一定惯性，即多次探索后别跑太远，跑丢了就不合适了。评价跑没跑丢，可以以当前参数的均值作为评判标准。
OU过程就是这样一个公式。如果t1时刻该值比均值，那下一时刻就会往均值靠一点，然后公式会再添加一个随机噪声，让参数不至于贴均值太紧。
里面两个超参分别是0.5和0.2，在11页第7节有写明，均值设为0。
总之，这个探索方法用上去好。
